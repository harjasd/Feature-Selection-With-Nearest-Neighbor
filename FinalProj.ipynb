{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Stub evaluation function that returns a random value\n",
        "def evaluate(features):\n",
        "    return random.uniform(0.25, 0.75) * 100  # returns a percentage value\n",
        "\n",
        "# Forward Selection with detailed trace\n",
        "def forward_selection(all_features):\n",
        "  #Algorithm starts with no selected features & best score of 0\n",
        "    selected_features = []\n",
        "    best_score = 0\n",
        "    improvement = True\n",
        "\n",
        "    print(\"\\nYou have selected Forward Selection\\n\")\n",
        "\n",
        "    # Initial random accuracy with no features\n",
        "    initial_score = evaluate(selected_features)\n",
        "    print(f\"Using no features and “random” evaluation, I get an accuracy of {initial_score:.1f}%\\n\")\n",
        "\n",
        "    iteration = 1\n",
        "    while improvement:\n",
        "        print(f\"Beginning search.\\n\")\n",
        "        improvement = False\n",
        "        best_candidate = None\n",
        "        candidates = []\n",
        "        for feature in all_features:\n",
        "            if feature not in selected_features:\n",
        "                candidate_features = selected_features + [feature]\n",
        "                score = evaluate(candidate_features)\n",
        "                candidates.append((feature, score))\n",
        "\n",
        "        for feature, score in candidates:\n",
        "            print(f\"Using feature(s) {selected_features + [feature]} accuracy is {score:.1f}%\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_candidate = feature\n",
        "                improvement = True\n",
        "\n",
        "        if best_candidate:\n",
        "            selected_features.append(best_candidate)\n",
        "            print(f\"\\nFeature set {selected_features} was best, accuracy is {best_score:.1f}%\\n\")\n",
        "        iteration += 1\n",
        "\n",
        "\n",
        "    print(f\"Finished search!! The best feature subset is {selected_features}, which has an accuracy of {best_score:.1f}%\")\n",
        "\n",
        "# Backward Elimination with detailed trace\n",
        "\n",
        "#Feature selection technique that iteratively removes the worst(least significant feature) to improve model\n",
        "def backward_elimination(all_features):\n",
        "  #initialize with all features and evaluate accuracy\n",
        "    selected_features = list(all_features)\n",
        "    best_score = evaluate(selected_features)\n",
        "\n",
        "    print(\"\\nYou have selected Backward Elimination\\n\")\n",
        "    print(f\"Using all features and “random” evaluation, I get an accuracy of {best_score:.1f}%\\n\")\n",
        "\n",
        "    improvement = True\n",
        "    while improvement:\n",
        "        print(f\"Beginning search.\\n\")\n",
        "        #reset improvement flag, worst candidate to none and initialize candidate list\n",
        "        improvement = False\n",
        "        worst_candidate = None\n",
        "        candidates = []\n",
        "\n",
        "        #iterate over selected features, evaluate accuracy with candidate set and append the score to the list\n",
        "        for feature in selected_features:\n",
        "            candidate_features = [f for f in selected_features if f != feature]\n",
        "            score = evaluate(candidate_features)\n",
        "            candidates.append((feature, score))\n",
        "\n",
        "            #iterate over features and their scores,\n",
        "        for feature, score in candidates:\n",
        "            print(f\"Using feature(s) {[f for f in selected_features if f != feature]} accuracy is {score:.1f}%\")\n",
        "            #if current candidate improves score, update best score, update worst candidate and set improvement flag\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                worst_candidate = feature\n",
        "                improvement = True\n",
        "\n",
        "        #if there is an improvement, remove the worst candidate feature\n",
        "        if worst_candidate:\n",
        "            selected_features.remove(worst_candidate)\n",
        "            print(f\"\\nFeature set {selected_features} was best, accuracy is {best_score:.1f}%\\n\")\n",
        "\n",
        "\n",
        "    print(f\"The best feature subset is {selected_features}, which has an accuracy of {best_score:.1f}%\")\n",
        "\n",
        "# Main program to ask user for input and run the chosen algorithm\n",
        "def main():\n",
        "    all_features = ['1', '2', '3', '4']\n",
        "\n",
        "    print(\"Welcome to Our CS170 Group's Feature Selection Algorithm.\")\n",
        "    print(f\"Please enter total number of features: {len(all_features)}\")\n",
        "    print(\"Type the number of the algorithm you want to run.\")\n",
        "    print(\"\\n1. Forward Selection\\n2. Backward Elimination\\n\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1 for Forward Selection, 2 for Backward Elimination): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        forward_selection(all_features)\n",
        "    elif choice == '2':\n",
        "        backward_elimination(all_features)\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojgeGSm8-jIU",
        "outputId": "d6500648-118a-45d9-eba6-084344cd872c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Our CS170 Group's Feature Selection Algorithm.\n",
            "Please enter total number of features: 4\n",
            "Type the number of the algorithm you want to run.\n",
            "\n",
            "1. Forward Selection\n",
            "2. Backward Elimination\n",
            "\n",
            "Enter your choice (1 for Forward Selection, 2 for Backward Elimination): 1\n",
            "\n",
            "You have selected Forward Selection\n",
            "\n",
            "Using no features and “random” evaluation, I get an accuracy of 54.8%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) ['1'] accuracy is 44.4%\n",
            "Using feature(s) ['2'] accuracy is 55.3%\n",
            "Using feature(s) ['3'] accuracy is 55.9%\n",
            "Using feature(s) ['4'] accuracy is 31.0%\n",
            "\n",
            "Feature set ['3'] was best, accuracy is 55.9%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) ['3', '1'] accuracy is 64.0%\n",
            "Using feature(s) ['3', '2'] accuracy is 74.0%\n",
            "Using feature(s) ['3', '4'] accuracy is 32.8%\n",
            "\n",
            "Feature set ['3', '2'] was best, accuracy is 74.0%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) ['3', '2', '1'] accuracy is 71.7%\n",
            "Using feature(s) ['3', '2', '4'] accuracy is 64.9%\n",
            "Finished search!! The best feature subset is ['3', '2'], which has an accuracy of 74.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "class NNClassifier:\n",
        "    def __init__(self):\n",
        "        self.training_data = None\n",
        "        self.training_labels = None\n",
        "\n",
        "    def train(self, training_data, training_labels):\n",
        "        \"\"\"Store the training data and labels.\"\"\"\n",
        "        self.training_data = training_data\n",
        "        self.training_labels = training_labels\n",
        "\n",
        "    def test(self, test_instance):\n",
        "        \"\"\"Return the class label of the nearest training instance.\"\"\"\n",
        "        if self.training_data is None or self.training_labels is None:\n",
        "            raise ValueError(\"The classifier has not been trained yet.\")\n",
        "\n",
        "        # Calculate Euclidean distances from the test instance to all training instances\n",
        "        distances = distance.cdist([test_instance], self.training_data, 'euclidean')\n",
        "        nearest_index = np.argmin(distances)\n",
        "        return self.training_labels[nearest_index]\n",
        "\n",
        "    def validate(self, feature_subset, data, labels):\n",
        "        \"\"\"Calculate the accuracy of the classifier given a specific feature subset.\"\"\"\n",
        "        if data is None or labels is None:\n",
        "            print(\"Unable to validate: Data or labels are missing.\")\n",
        "            return 0.0\n",
        "\n",
        "        selected_data = data[:, feature_subset]\n",
        "        num_instances = selected_data.shape[0]\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            # Leave-one-out validation: Train on all instances except the i-th one and test on the i-th one\n",
        "            train_data = np.delete(selected_data, i, axis=0)\n",
        "            train_labels = np.delete(labels, i, axis=0)\n",
        "            test_instance = selected_data[i]\n",
        "            true_label = labels[i]\n",
        "\n",
        "            self.train(train_data, train_labels)\n",
        "            predicted_label = self.test(test_instance)\n",
        "\n",
        "            if predicted_label == true_label:\n",
        "                correct_predictions += 1\n",
        "\n",
        "        accuracy = (correct_predictions / num_instances) * 100\n",
        "        return accuracy\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        # Load the entire dataset\n",
        "        data = np.genfromtxt(file_path)\n",
        "\n",
        "        # Extract labels and features separately\n",
        "        labels = data[:, 0].astype(int)\n",
        "        features = data[:, 1:]\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load small test dataset\n",
        "    small_dataset_path = '/content/small-test-dataset.txt'\n",
        "    data, labels = load_dataset(small_dataset_path)\n",
        "    feature_subset = [2, 4, 6]  # zero-indexed for features {3, 5, 7}\n",
        "\n",
        "    classifier = NNClassifier()\n",
        "    accuracy = classifier.validate(feature_subset, data, labels)\n",
        "\n",
        "    print(f\"Accuracy of NN classifier with feature subset {feature_subset} on small dataset: {accuracy:.2f}%\")\n",
        "\n",
        "    # Load large test dataset\n",
        "    large_dataset_path = '/content/large-test-dataset.txt'\n",
        "    data, labels = load_dataset(large_dataset_path)\n",
        "    feature_subset = [0, 14, 26]  # zero-indexed for features {1, 15, 27}\n",
        "\n",
        "    accuracy = classifier.validate(feature_subset, data, labels)\n",
        "\n",
        "    print(f\"Accuracy of NN classifier with feature subset {feature_subset} on large dataset: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urDtFjtkOBHd",
        "outputId": "3a4593c6-fdc8-4dcd-c599-804c1d294ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of NN classifier with feature subset [2, 4, 6] on small dataset: 89.00%\n",
            "Accuracy of NN classifier with feature subset [0, 14, 26] on large dataset: 94.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "class NNClassifier:\n",
        "    def __init__(self):\n",
        "        self.training_data = None\n",
        "        self.training_labels = None\n",
        "\n",
        "    def train(self, training_data, training_labels):\n",
        "        \"\"\"Store the training data and labels.\"\"\"\n",
        "        self.training_data = training_data\n",
        "        self.training_labels = training_labels\n",
        "\n",
        "    def test(self, test_instance):\n",
        "        \"\"\"Return the class label of the nearest training instance.\"\"\"\n",
        "        if self.training_data is None or self.training_labels is None:\n",
        "            raise ValueError(\"The classifier has not been trained yet.\")\n",
        "\n",
        "        # Calculate Euclidean distances from the test instance to all training instances\n",
        "        distances = distance.cdist([test_instance], self.training_data, 'euclidean')\n",
        "        nearest_index = np.argmin(distances)\n",
        "        return self.training_labels[nearest_index]\n",
        "\n",
        "    def validate(self, feature_subset, data, labels):\n",
        "        \"\"\"Calculate the accuracy of the classifier given a specific feature subset.\"\"\"\n",
        "        if data is None or labels is None:\n",
        "            print(\"Unable to validate: Data or labels are missing.\")\n",
        "            return 0.0\n",
        "\n",
        "        selected_data = data[:, feature_subset]\n",
        "        num_instances = selected_data.shape[0]\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            # Leave-one-out validation: Train on all instances except the i-th one and test on the i-th one\n",
        "            train_data = np.delete(selected_data, i, axis=0)\n",
        "            train_labels = np.delete(labels, i, axis=0)\n",
        "            test_instance = selected_data[i]\n",
        "            true_label = labels[i]\n",
        "\n",
        "            self.train(train_data, train_labels)\n",
        "            predicted_label = self.test(test_instance)\n",
        "\n",
        "            if predicted_label == true_label:\n",
        "                correct_predictions += 1\n",
        "\n",
        "        accuracy = (correct_predictions / num_instances) * 100\n",
        "        return accuracy\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        # Load the entire dataset\n",
        "        data = np.genfromtxt(file_path)\n",
        "\n",
        "        # Extract labels and features separately\n",
        "        labels = data[:, 0].astype(int)\n",
        "        features = data[:, 1:]\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Updated evaluation function using NNClassifier\n",
        "def evaluate(feature_subset, features, labels):\n",
        "    classifier = NNClassifier()\n",
        "    classifier.train(features, labels)  # Initialize the classifier with training data and labels\n",
        "    return classifier.validate(feature_subset, features, labels)\n",
        "\n",
        "# Forward Selection with detailed trace\n",
        "def forward_selection(all_features, features, labels):\n",
        "    selected_features = []\n",
        "    best_score = 0\n",
        "\n",
        "    print(\"\\nYou have selected Forward Selection\\n\")\n",
        "\n",
        "    # Initial accuracy with no features\n",
        "    initial_score = evaluate([], features, labels)\n",
        "    print(f\"Using no features and evaluation, I get an accuracy of {initial_score:.1f}%\\n\")\n",
        "\n",
        "    while True:\n",
        "        print(\"Beginning search.\\n\")\n",
        "        improvement = False\n",
        "        best_candidate = None\n",
        "        candidates = []\n",
        "\n",
        "        for feature in all_features:\n",
        "            if feature not in selected_features:\n",
        "                candidate_features = selected_features + [feature]\n",
        "                score = evaluate(candidate_features, features, labels)\n",
        "                candidates.append((feature, score))\n",
        "\n",
        "        for feature, score in candidates:\n",
        "            print(f\"Using feature(s) {selected_features + [feature]} accuracy is {score:.1f}%\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_candidate = feature\n",
        "                improvement = True\n",
        "\n",
        "        if improvement:\n",
        "            selected_features.append(best_candidate)\n",
        "            print(f\"\\nFeature set {selected_features} was best, accuracy is {best_score:.1f}%\\n\")\n",
        "        else:\n",
        "            print(\"No improvement in accuracy. Exiting...\\n\")\n",
        "            break\n",
        "\n",
        "    print(f\"Finished search!! The best feature subset is {selected_features}, which has an accuracy of {best_score:.1f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Backward Elimination with detailed trace\n",
        "def backward_elimination(all_features, features, labels):\n",
        "    selected_features = list(all_features)\n",
        "    best_score = evaluate(selected_features, features, labels)\n",
        "\n",
        "    print(\"\\nYou have selected Backward Elimination\\n\")\n",
        "    print(f\"Using all features and evaluation, I get an accuracy of {best_score:.1f}%\\n\")\n",
        "\n",
        "    improvement = True\n",
        "    while improvement:\n",
        "        print(\"Beginning search.\\n\")\n",
        "        improvement = False\n",
        "        worst_candidate = None\n",
        "        candidates = []\n",
        "\n",
        "        for feature in selected_features:\n",
        "            candidate_features = [f for f in selected_features if f != feature]\n",
        "            score = evaluate(candidate_features, features, labels)\n",
        "            candidates.append((feature, score))\n",
        "\n",
        "        for feature, score in candidates:\n",
        "            print(f\"Using feature(s) {[f for f in selected_features if f != feature]} accuracy is {score:.1f}%\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                worst_candidate = feature\n",
        "                improvement = True\n",
        "\n",
        "        if worst_candidate:\n",
        "            selected_features.remove(worst_candidate)\n",
        "            print(f\"\\nFeature set {selected_features} was best, accuracy is {best_score:.1f}%\\n\")\n",
        "\n",
        "    print(f\"The best feature subset is {selected_features}, which has an accuracy of {best_score:.1f}%\")\n",
        "\n",
        "# Main program to ask user for input and run the chosen algorithm\n",
        "def main():\n",
        "    small_dataset_path = '/content/CS170_Spring_2024_Small_data__63.txt'\n",
        "    large_dataset_path = '/content/CS170_Spring_2024_Large_data__63.txt'\n",
        "\n",
        "    features1, labels1 = load_dataset(small_dataset_path)\n",
        "    features2, labels2 = load_dataset(large_dataset_path)\n",
        "\n",
        "    if features1 is None or labels1 is None or features2 is None or labels2 is None:\n",
        "        return\n",
        "\n",
        "    print(\"Welcome to Our CS170 Group's Feature Selection Algorithm.\")\n",
        "    print(\"Please select the dataset you want to use:\")\n",
        "    print(\"1. Small Dataset\")\n",
        "    print(\"2. Large Dataset\")\n",
        "\n",
        "    dataset_choice = input(\"Enter your choice (1 for Small Dataset, 2 for Large Dataset): \").strip()\n",
        "\n",
        "    if dataset_choice == '1':\n",
        "        features = features1\n",
        "        labels = labels1\n",
        "    elif dataset_choice == '2':\n",
        "        features = features2\n",
        "        labels = labels2\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
        "        return\n",
        "\n",
        "    all_features = list(range(features.shape[1]))\n",
        "\n",
        "    print(f\"Please enter total number of features: {len(all_features)}\")\n",
        "    print(\"Type the number of the algorithm you want to run.\")\n",
        "    print(\"\\n1. Forward Selection\\n2. Backward Elimination\\n\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1 for Forward Selection, 2 for Backward Elimination): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        forward_selection(all_features, features, labels)\n",
        "    elif choice == '2':\n",
        "        backward_elimination(all_features, features, labels)\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OqqDCaUhdX4",
        "outputId": "7584f269-35bc-48a6-a877-18d97739495b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Our CS170 Group's Feature Selection Algorithm.\n",
            "Please select the dataset you want to use:\n",
            "1. Small Dataset\n",
            "2. Large Dataset\n",
            "Enter your choice (1 for Small Dataset, 2 for Large Dataset): 2\n",
            "Please enter total number of features: 40\n",
            "Type the number of the algorithm you want to run.\n",
            "\n",
            "1. Forward Selection\n",
            "2. Backward Elimination\n",
            "\n",
            "Enter your choice (1 for Forward Selection, 2 for Backward Elimination): 1\n",
            "\n",
            "You have selected Forward Selection\n",
            "\n",
            "Using no features and evaluation, I get an accuracy of 83.5%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) [0] accuracy is 70.1%\n",
            "Using feature(s) [1] accuracy is 73.8%\n",
            "Using feature(s) [2] accuracy is 71.5%\n",
            "Using feature(s) [3] accuracy is 72.5%\n",
            "Using feature(s) [4] accuracy is 72.5%\n",
            "Using feature(s) [5] accuracy is 70.5%\n",
            "Using feature(s) [6] accuracy is 72.5%\n",
            "Using feature(s) [7] accuracy is 72.3%\n",
            "Using feature(s) [8] accuracy is 71.8%\n",
            "Using feature(s) [9] accuracy is 74.9%\n",
            "Using feature(s) [10] accuracy is 69.9%\n",
            "Using feature(s) [11] accuracy is 71.5%\n",
            "Using feature(s) [12] accuracy is 73.3%\n",
            "Using feature(s) [13] accuracy is 72.4%\n",
            "Using feature(s) [14] accuracy is 72.6%\n",
            "Using feature(s) [15] accuracy is 73.0%\n",
            "Using feature(s) [16] accuracy is 71.2%\n",
            "Using feature(s) [17] accuracy is 85.3%\n",
            "Using feature(s) [18] accuracy is 72.8%\n",
            "Using feature(s) [19] accuracy is 73.4%\n",
            "Using feature(s) [20] accuracy is 72.2%\n",
            "Using feature(s) [21] accuracy is 72.3%\n",
            "Using feature(s) [22] accuracy is 73.0%\n",
            "Using feature(s) [23] accuracy is 72.5%\n",
            "Using feature(s) [24] accuracy is 72.2%\n",
            "Using feature(s) [25] accuracy is 71.3%\n",
            "Using feature(s) [26] accuracy is 72.2%\n",
            "Using feature(s) [27] accuracy is 74.2%\n",
            "Using feature(s) [28] accuracy is 70.3%\n",
            "Using feature(s) [29] accuracy is 72.9%\n",
            "Using feature(s) [30] accuracy is 73.7%\n",
            "Using feature(s) [31] accuracy is 72.8%\n",
            "Using feature(s) [32] accuracy is 72.2%\n",
            "Using feature(s) [33] accuracy is 72.5%\n",
            "Using feature(s) [34] accuracy is 71.8%\n",
            "Using feature(s) [35] accuracy is 71.9%\n",
            "Using feature(s) [36] accuracy is 70.6%\n",
            "Using feature(s) [37] accuracy is 72.3%\n",
            "Using feature(s) [38] accuracy is 71.4%\n",
            "Using feature(s) [39] accuracy is 70.4%\n",
            "\n",
            "Feature set [17] was best, accuracy is 85.3%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) [17, 0] accuracy is 83.7%\n",
            "Using feature(s) [17, 1] accuracy is 85.2%\n",
            "Using feature(s) [17, 2] accuracy is 84.9%\n",
            "Using feature(s) [17, 3] accuracy is 84.0%\n",
            "Using feature(s) [17, 4] accuracy is 82.9%\n",
            "Using feature(s) [17, 5] accuracy is 84.8%\n",
            "Using feature(s) [17, 6] accuracy is 86.1%\n",
            "Using feature(s) [17, 7] accuracy is 86.4%\n",
            "Using feature(s) [17, 8] accuracy is 85.6%\n",
            "Using feature(s) [17, 9] accuracy is 97.0%\n",
            "Using feature(s) [17, 10] accuracy is 84.4%\n",
            "Using feature(s) [17, 11] accuracy is 83.8%\n",
            "Using feature(s) [17, 12] accuracy is 85.5%\n",
            "Using feature(s) [17, 13] accuracy is 84.9%\n",
            "Using feature(s) [17, 14] accuracy is 84.6%\n",
            "Using feature(s) [17, 15] accuracy is 83.9%\n",
            "Using feature(s) [17, 16] accuracy is 83.2%\n",
            "Using feature(s) [17, 18] accuracy is 84.8%\n",
            "Using feature(s) [17, 19] accuracy is 84.3%\n",
            "Using feature(s) [17, 20] accuracy is 84.0%\n",
            "Using feature(s) [17, 21] accuracy is 84.4%\n",
            "Using feature(s) [17, 22] accuracy is 85.7%\n",
            "Using feature(s) [17, 23] accuracy is 85.9%\n",
            "Using feature(s) [17, 24] accuracy is 83.7%\n",
            "Using feature(s) [17, 25] accuracy is 85.6%\n",
            "Using feature(s) [17, 26] accuracy is 84.4%\n",
            "Using feature(s) [17, 27] accuracy is 84.5%\n",
            "Using feature(s) [17, 28] accuracy is 86.0%\n",
            "Using feature(s) [17, 29] accuracy is 84.4%\n",
            "Using feature(s) [17, 30] accuracy is 83.8%\n",
            "Using feature(s) [17, 31] accuracy is 85.8%\n",
            "Using feature(s) [17, 32] accuracy is 84.0%\n",
            "Using feature(s) [17, 33] accuracy is 84.6%\n",
            "Using feature(s) [17, 34] accuracy is 84.7%\n",
            "Using feature(s) [17, 35] accuracy is 84.3%\n",
            "Using feature(s) [17, 36] accuracy is 84.1%\n",
            "Using feature(s) [17, 37] accuracy is 84.6%\n",
            "Using feature(s) [17, 38] accuracy is 83.8%\n",
            "Using feature(s) [17, 39] accuracy is 83.8%\n",
            "\n",
            "Feature set [17, 9] was best, accuracy is 97.0%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) [17, 9, 0] accuracy is 93.3%\n",
            "Using feature(s) [17, 9, 1] accuracy is 93.5%\n",
            "Using feature(s) [17, 9, 2] accuracy is 93.6%\n",
            "Using feature(s) [17, 9, 3] accuracy is 92.3%\n",
            "Using feature(s) [17, 9, 4] accuracy is 92.9%\n",
            "Using feature(s) [17, 9, 5] accuracy is 93.1%\n",
            "Using feature(s) [17, 9, 6] accuracy is 94.1%\n",
            "Using feature(s) [17, 9, 7] accuracy is 92.5%\n",
            "Using feature(s) [17, 9, 8] accuracy is 94.0%\n",
            "Using feature(s) [17, 9, 10] accuracy is 92.7%\n",
            "Using feature(s) [17, 9, 11] accuracy is 92.5%\n",
            "Using feature(s) [17, 9, 12] accuracy is 95.1%\n",
            "Using feature(s) [17, 9, 13] accuracy is 93.3%\n",
            "Using feature(s) [17, 9, 14] accuracy is 92.7%\n",
            "Using feature(s) [17, 9, 15] accuracy is 91.8%\n",
            "Using feature(s) [17, 9, 16] accuracy is 93.7%\n",
            "Using feature(s) [17, 9, 18] accuracy is 93.9%\n",
            "Using feature(s) [17, 9, 19] accuracy is 92.3%\n",
            "Using feature(s) [17, 9, 20] accuracy is 92.2%\n",
            "Using feature(s) [17, 9, 21] accuracy is 92.9%\n",
            "Using feature(s) [17, 9, 22] accuracy is 93.3%\n",
            "Using feature(s) [17, 9, 23] accuracy is 95.1%\n",
            "Using feature(s) [17, 9, 24] accuracy is 92.5%\n",
            "Using feature(s) [17, 9, 25] accuracy is 93.6%\n",
            "Using feature(s) [17, 9, 26] accuracy is 93.8%\n",
            "Using feature(s) [17, 9, 27] accuracy is 93.3%\n",
            "Using feature(s) [17, 9, 28] accuracy is 94.3%\n",
            "Using feature(s) [17, 9, 29] accuracy is 92.7%\n",
            "Using feature(s) [17, 9, 30] accuracy is 93.4%\n",
            "Using feature(s) [17, 9, 31] accuracy is 91.4%\n",
            "Using feature(s) [17, 9, 32] accuracy is 92.7%\n",
            "Using feature(s) [17, 9, 33] accuracy is 93.0%\n",
            "Using feature(s) [17, 9, 34] accuracy is 93.5%\n",
            "Using feature(s) [17, 9, 35] accuracy is 92.2%\n",
            "Using feature(s) [17, 9, 36] accuracy is 93.0%\n",
            "Using feature(s) [17, 9, 37] accuracy is 93.9%\n",
            "Using feature(s) [17, 9, 38] accuracy is 92.8%\n",
            "Using feature(s) [17, 9, 39] accuracy is 92.9%\n",
            "No improvement in accuracy. Exiting...\n",
            "\n",
            "Finished search!! The best feature subset is [17, 9], which has an accuracy of 97.0%\n"
          ]
        }
      ]
    }
  ]
}